{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d90d895-dde4-4738-a100-9ccab57d5f79",
   "metadata": {},
   "source": [
    "# Trabajo Final - Lenguaje de Programaci√≥n 2\n",
    "### T√≠tulo del Proyecto\n",
    "**Sistema de Monitoreo y Comparaci√≥n de Precios de Hardware (Smarth Shop)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b3c35-fbdd-450d-8247-6ada3407fb8a",
   "metadata": {},
   "source": [
    "### Nota\n",
    "\n",
    "Todo el desarrollo del proyecto se ha realizado respetando las directivas del archivo robots.txt y los t√©rminos de servicio de las p√°ginas web y APIs consultadas. Asimismo, se implementaron tiempos de espera (delays) entre las solicitudes para evitar la sobrecarga de los servidores externos y asegurar una extracci√≥n responsable de la informaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214f854-e438-4e8e-bb30-198c146dbaf6",
   "metadata": {},
   "source": [
    "### Introducci√≥n y Relevancia del Proyecto\n",
    "\n",
    "En un contexto econ√≥mico marcado por la inflaci√≥n y la constante variaci√≥n de precios, los consumidores peruanos enfrentan dificultades para identificar ofertas reales en productos tecnol√≥gicos como laptops y componentes de hardware. Muchas veces, las promociones mostradas en tiendas virtuales no reflejan un verdadero ahorro, lo que genera desinformaci√≥n y decisiones de compra poco √≥ptimas.\n",
    "\n",
    "El proyecto **Smart Shop** surge como una soluci√≥n tecnol√≥gica orientada a centralizar, comparar y normalizar precios de productos de hardware provenientes de distintas tiendas, permitiendo a los usuarios identificar el precio m√°s bajo disponible en el mercado al momento de la consulta. De esta manera, el sistema contribuye al ahorro econ√≥mico, fomenta decisiones de compra informadas y protege el poder adquisitivo de personas que dependen de la tecnolog√≠a para estudiar, trabajar o emprender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0466eba7-821b-46a1-ba01-3b0a094b0ca2",
   "metadata": {},
   "source": [
    "### Objetivos del Proyecto\n",
    "#### Objetivo General\n",
    "\n",
    "Desarrollar un sistema automatizado en Python que permita monitorear y comparar precios de productos tecnol√≥gicos entre diferentes retailers, presentando la informaci√≥n de forma estructurada y estandarizada.\n",
    "\n",
    "#### Objetivos Espec√≠ficos\n",
    "\n",
    "- Desarrollar un bot en Python capaz de extraer diariamente precios de laptops y componentes de hardware desde tiendas reconocidas.\n",
    "- Implementar la normalizaci√≥n de moneda (USD a PEN) en tiempo real mediante el consumo de una API de tipo de cambio.\n",
    "- Generar un dataset estructurado en formato CSV que permita analizar y visualizar la dispersi√≥n de precios de un mismo producto.\n",
    "- Facilitar la identificaci√≥n del producto m√°s econ√≥mico disponible en la fecha de ejecuci√≥n del sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b67fd01-9a33-43b2-afe8-8525e38ec77a",
   "metadata": {},
   "source": [
    "### Fuentes de Datos y Estrategia de Extracci√≥n\n",
    "\n",
    "El proyecto utiliza **tres tipos de fuentes de datos**, cumpliendo con los requisitos del curso y aplicando distintas t√©cnicas de adquisici√≥n de informaci√≥n:\n",
    "\n",
    "#### Fuente 1: Web Scraping Est√°tico\n",
    "- **Amazon**\n",
    "- Se emplea la librer√≠a **BeautifulSoup** para extraer informaci√≥n de cat√°logos de laptops.\n",
    "- Los datos recolectados incluyen: nombre del producto, precio, descripci√≥n y vendedor.\n",
    "- Esta fuente permite obtener informaci√≥n estructurada directamente desde el HTML est√°tico.\n",
    "\n",
    "#### Fuente 2: Web Scraping Din√°mico\n",
    "- **Plaza Vea, Coolbox y Falabella**\n",
    "- Se utiliza **Selenium** para renderizar contenido din√°mico generado mediante JavaScript.\n",
    "- Se extraen precios y ofertas exclusivas disponibles √∫nicamente en la web.\n",
    "- Esta t√©cnica permite acceder a informaci√≥n que no est√° disponible mediante scraping tradicional.\n",
    "\n",
    "#### Fuente 3: API P√∫blica\n",
    "- **ExchangeRate-API**\n",
    "- Se consume una API REST para obtener el tipo de cambio actualizado entre d√≥lares estadounidenses (USD) y soles peruanos (PEN).\n",
    "- Esta informaci√≥n es clave para estandarizar todos los precios recolectados y permitir una comparaci√≥n precisa entre tiendas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631d3e2-4b77-4ed1-af33-e16b35122030",
   "metadata": {},
   "source": [
    "### Integraci√≥n y Tratamiento de Datos\n",
    "\n",
    "Una vez obtenidos los datos desde las distintas fuentes, el sistema realiza un proceso de integraci√≥n y limpieza que incluye:\n",
    "- Conversi√≥n de todos los precios a moneda local (PEN).\n",
    "- Normalizaci√≥n de nombres de productos para facilitar la comparaci√≥n.\n",
    "- Eliminaci√≥n de registros duplicados.\n",
    "- Manejo de valores nulos o inconsistentes.\n",
    "\n",
    "El resultado de este proceso es un dataset limpio y estructurado, listo para su an√°lisis posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fda0ba-bc63-4d86-8a65-dc912f1771b3",
   "metadata": {},
   "source": [
    "### Producto Final\n",
    "\n",
    "El producto final del proyecto consiste en:\n",
    "- Un **repositorio en GitHub** que contiene todo el c√≥digo fuente, documentado y organizado por m√≥dulos.\n",
    "- Un **archivo CSV/Excel** que presenta la comparaci√≥n de precios de los productos analizados.\n",
    "- Un reporte que permite identificar claramente cu√°l es el producto m√°s barato del mercado en la fecha de ejecuci√≥n del sistema.\n",
    "\n",
    "Este sistema puede servir como base para futuras extensiones, como visualizaciones interactivas, alertas de precios o una aplicaci√≥n web orientada al consumidor final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63764d-cd8c-43c7-85ab-8a74da346871",
   "metadata": {},
   "source": [
    "### Tecnolog√≠as Utilizadas\n",
    "- **Lenguaje:** Python 3.x\n",
    "- **Librer√≠as principales:**\n",
    "    - Requests\n",
    "    - BeautifulSoup4\n",
    "    - Selenium\n",
    "    - Pandas\n",
    "\n",
    "- **Formato de salida:** CSV / Excel\n",
    "- **Control de versiones:** Git y GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d607b0-0080-4cc8-8bbe-0be7b728cd3a",
   "metadata": {},
   "source": [
    "### Importaci√≥n de librer√≠as necesarias para la extracci√≥n, procesamiento y almacenamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585bc11-6a7c-4dc0-910c-634f388ec937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19fc21a-0af9-4a0e-9d78-1ee2e8b29f08",
   "metadata": {},
   "source": [
    "### Configuraci√≥n de par√°metros para Amazon y definici√≥n de funci√≥n de limpieza de precios\n",
    "En esta secci√≥n se definen los par√°metros base para realizar el scraping de productos desde Amazon, incluyendo el t√©rmino de b√∫squeda, la cantidad de p√°ginas a recorrer y las cabeceras HTTP necesarias para simular un navegador real. Asimismo, se implementa una funci√≥n auxiliar para la limpieza y normalizaci√≥n de precios, la cual ser√° utilizada en etapas posteriores del proceso de extracci√≥n y an√°lisis de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38871b-a393-4636-8920-7ba5dcd86f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURACI√ìN INICIAL PARA AMAZON ---\n",
    "TERMINO_BUSQUEDA = \"computadores\"\n",
    "URL_INICIAL = f\"https://www.amazon.com/s?k={TERMINO_BUSQUEDA}\" \n",
    "NUMERO_DE_PAGINAS = 5  # Extracci√≥n de 5 p√°ginas\n",
    "\n",
    "\n",
    "# Headers avanzados para simular un navegador genuino y reducir\n",
    "# la probabilidad de bloqueos durante el scraping\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Referer': 'https://www.amazon.com/', \n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'es-ES,es;q=0.9',\n",
    "    'Cookie': 'custom_cookie=true',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "\n",
    "# --- FUNCI√ìN AUXILIAR PARA LIMPIEZA DE PRECIOS ---\n",
    "# Esta funci√≥n permite transformar los precios extra√≠dos desde\n",
    "# la web (en formato texto) a un formato num√©rico est√°ndar,\n",
    "# eliminando s√≠mbolos monetarios, separadores de miles y texto\n",
    "# irrelevante, con el fin de permitir c√°lculos y comparaciones.\n",
    "def limpiar_precio(precio_str):\n",
    "    \"\"\"\n",
    "    Limpia y normaliza el precio extra√≠do como texto, eliminando\n",
    "    s√≠mbolos monetarios, texto adicional y separadores innecesarios,\n",
    "    garantizando un formato num√©rico adecuado para an√°lisis posterior.\n",
    "\n",
    "    Par√°metros:\n",
    "    precio_str (str): Precio en formato texto obtenido durante el scraping.\n",
    "\n",
    "    Retorna:\n",
    "    float | None: Precio convertido a tipo float o None si la conversi√≥n falla.\n",
    "    \"\"\"\n",
    "    if isinstance(precio_str, str):\n",
    "        # 1. Eliminar caracteres no deseados y texto de referencia\n",
    "        precio_limpio = precio_str.replace('\\u00a0', ' ').replace('USD', '').replace('PEN', '').replace('S/', '').replace('$', '').replace('PVPR:', '').replace('Lista:', '').strip()\n",
    "        \n",
    "        # 2. Manejar separadores: eliminar comas utilizadas como separadores de miles\n",
    "        precio_limpio = precio_limpio.replace(',', '') \n",
    "        \n",
    "        # 3. Forzar formato de un solo punto decimal, eliminando puntos adicionales\n",
    "        if precio_limpio.count('.') > 1:\n",
    "            partes = precio_limpio.rsplit('.', 1) \n",
    "            entero = partes[0].replace('.', '') \n",
    "            decimal = partes[1] if len(partes) > 1 else '00'\n",
    "            precio_limpio = f\"{entero}.{decimal}\"\n",
    "        \n",
    "        # 4. Asegurar que no exista doble punto residual\n",
    "        precio_limpio = precio_limpio.replace('..', '.')\n",
    "        \n",
    "        try:\n",
    "            # 5. Conversi√≥n final a tipo float\n",
    "            return float(precio_limpio.strip())\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991a860-181a-44cd-9a7a-653332ac42d1",
   "metadata": {},
   "source": [
    "### FUNCI√ìN DE SOLICITUD HTTP Y PARSEO DEL CONTENIDO HTML\n",
    "Esta funci√≥n centraliza el proceso de conexi√≥n a la p√°gina web, realizando la solicitud HTTP y transformando la respuesta en un objeto BeautifulSoup. De esta manera, se desacopla la l√≥gica de descarga del contenido de la l√≥gica de extracci√≥n de datos, facilitando la reutilizaci√≥n del c√≥digo y el mantenimiento del sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6424fd-1b2b-46c2-ae9b-3220cdff7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. FUNCI√ìN DE SOLICITUD HTTP Y PARSEO ---\n",
    "def obtener_contenido_pagina(url):\n",
    "    \"\"\"\n",
    "    Realiza una solicitud HTTP GET a la URL indicada y devuelve el\n",
    "    contenido HTML parseado como un objeto BeautifulSoup.\n",
    "\n",
    "    Esta funci√≥n incorpora manejo de errores y validaci√≥n del c√≥digo\n",
    "    de estado HTTP, asegurando que solo se procese contenido v√°lido\n",
    "    durante la etapa de scraping.\n",
    "\n",
    "    Par√°metros:\n",
    "    url (str): Direcci√≥n web de la p√°gina a consultar.\n",
    "\n",
    "    Retorna:\n",
    "    BeautifulSoup | None: Objeto BeautifulSoup con el HTML parseado\n",
    "    o None si ocurre un error durante la solicitud.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        \n",
    "        # Mostrar el c√≥digo de estado HTTP para fines de monitoreo\n",
    "        print(f\"    - C√≥digo de estado HTTP recibido: {response.status_code}\") \n",
    "        \n",
    "        # Lanza una excepci√≥n si la respuesta HTTP indica error\n",
    "        response.raise_for_status() \n",
    "\n",
    "        # Parseo del contenido HTML con BeautifulSoup\n",
    "        return BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Manejo de errores de conexi√≥n, timeout o respuesta inv√°lida\n",
    "        print(f\"‚ùå Error al realizar la solicitud a {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10564be1-5ffe-4fa2-ac33-49276f4dedd8",
   "metadata": {},
   "source": [
    "### FUNCI√ìN DE EXTRACCI√ìN, FILTRADO Y PROCESAMIENTO DE OFERTAS\n",
    "Esta funci√≥n se encarga de recorrer el contenido HTML previamente parseado de Amazon y extraer informaci√≥n relevante √∫nicamente de productos que presentan una oferta real (precio anterior y precio actual). Se aplica un filtrado estricto para garantizar que los datos obtenidos sean consistentes y √∫tiles para el an√°lisis comparativo de precios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f1d397-1c86-463f-ae1e-93aee36108ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. FUNCI√ìN DE EXTRACCI√ìN Y FILTRADO ESTRICTO ---\n",
    "def extraer_datos_amazon_ofertas(soup):\n",
    "    \"\"\"\n",
    "    Extrae informaci√≥n de productos en oferta desde el HTML de Amazon,\n",
    "    aplicando un filtrado estricto para conservar √∫nicamente aquellos\n",
    "    productos que cuentan con nombre, precio anterior y precio actual.\n",
    "\n",
    "    La funci√≥n tambi√©n calcula el porcentaje de descuento utilizando\n",
    "    valores num√©ricos normalizados, garantizando consistencia para\n",
    "    an√°lisis posteriores.\n",
    "\n",
    "    Par√°metros:\n",
    "    soup (BeautifulSoup): Objeto BeautifulSoup con el HTML parseado\n",
    "    de la p√°gina de resultados de Amazon.\n",
    "\n",
    "    Retorna:\n",
    "    list: Lista de diccionarios con la informaci√≥n estructurada de\n",
    "    cada producto v√°lido encontrado.\n",
    "    \"\"\"\n",
    "    datos_productos = []\n",
    "\n",
    "    # Selector principal de contenedores de productos\n",
    "    contenedores_productos = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "    \n",
    "    # Mensaje informativo sobre la cantidad de productos encontrados\n",
    "    print(f\"    -> Productos encontrados para extraer: {len(contenedores_productos)}\")\n",
    "\n",
    "    for contenedor in contenedores_productos:\n",
    "        \n",
    "        # Inicializaci√≥n de variables con valores por defecto\n",
    "        nombre = 'N/A'\n",
    "        precio_antes = 'N/A'\n",
    "        precio_despues = 'N/A'\n",
    "        url_image = 'N/A'\n",
    "        \n",
    "        # --- Extracci√≥n del nombre del producto ---\n",
    "        titulo_h2 = contenedor.find('h2')\n",
    "        if titulo_h2:\n",
    "            span_titulo = titulo_h2.find('span')\n",
    "            nombre = span_titulo.text.strip() if span_titulo else titulo_h2.text.strip()\n",
    "        \n",
    "        # --- Extracci√≥n del precio actual (precio_despues) ---\n",
    "        precio_span = contenedor.find('span', class_='a-price')\n",
    "        if precio_span:\n",
    "            p_entero = precio_span.find('span', class_='a-price-whole')\n",
    "            p_fraccion = precio_span.find('span', class_='a-price-fraction')\n",
    "            moneda = precio_span.find('span', class_='a-price-symbol')\n",
    "            \n",
    "            p_entero_str = p_entero.text.strip() if p_entero else ''\n",
    "            p_fraccion_str = p_fraccion.text.strip() if p_fraccion else ''\n",
    "            moneda_str = moneda.text.strip() if moneda else 'USD'\n",
    "            \n",
    "            if p_entero_str or p_fraccion_str:\n",
    "                # Construcci√≥n del precio en formato texto limpio\n",
    "                precio_despues = f\"{moneda_str} {p_entero_str}.{p_fraccion_str}\".replace('\\xa0', ' ').replace('..', '.') \n",
    "\n",
    "        # --- Extracci√≥n del precio anterior (precio_antes) ---\n",
    "        precio_antes_tag = contenedor.find('span', class_='a-price', attrs={'data-a-strike': 'true'})\n",
    "        \n",
    "        if precio_antes_tag:\n",
    "            offscreen_price = precio_antes_tag.find('span', class_='a-offscreen')\n",
    "            if offscreen_price:\n",
    "                precio_antes = offscreen_price.text.strip().replace('\\u00a0', ' ')\n",
    "            else:\n",
    "                precio_antes = precio_antes_tag.text.strip().replace('\\u00a0', ' ')\n",
    "            \n",
    "            # Limpieza adicional del texto del precio anterior\n",
    "            precio_antes = precio_antes.replace('PVPR:', '').replace('Lista:', '').strip().replace('..', '.')\n",
    "        \n",
    "        # --- Extracci√≥n de la URL de la imagen del producto ---\n",
    "        imagen_tag = contenedor.find('img', class_='s-image')\n",
    "        url_image = imagen_tag.get('src') if imagen_tag and imagen_tag.get('src') else 'N/A'\n",
    "        \n",
    "        # --- C√°lculo del descuento ---\n",
    "        descuento = '0%'\n",
    "        \n",
    "        # Conversi√≥n de precios a formato num√©rico para el c√°lculo\n",
    "        num_despues = limpiar_precio(precio_despues)\n",
    "        num_antes = limpiar_precio(precio_antes) \n",
    "        \n",
    "        if num_antes and num_despues and num_antes > num_despues:\n",
    "            calc_descuento = ((num_antes - num_despues) / num_antes) * 100\n",
    "            descuento = f\"{calc_descuento:.0f}%\" \n",
    "        \n",
    "        # --- FILTRO ESTRICTO ---\n",
    "        # Solo se agregan productos con nombre, precio actual y precio anterior v√°lidos\n",
    "        if nombre != 'N/A' and precio_despues != 'N/A' and precio_antes != 'N/A':\n",
    "            datos_productos.append({\n",
    "                \"nombre\": nombre.replace('\\u00a0', ' '),\n",
    "                \"precio_antes\": precio_antes.replace('\\u00a0', ' '),\n",
    "                \"precio_despues\": precio_despues.replace('\\u00a0', ' '), \n",
    "                \"descuento\": descuento,\n",
    "                \"url_image\": url_image\n",
    "            })\n",
    "        \n",
    "    return datos_productos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eee6954-d58b-4d2c-95b7-bac9b40583b6",
   "metadata": {},
   "source": [
    "### FUNCI√ìN PRINCIPAL DE ORQUESTACI√ìN Y PAGINACI√ìN DIN√ÅMICA\n",
    "Esta funci√≥n act√∫a como el controlador principal del scraper, coordinando la descarga de m√∫ltiples p√°ginas de resultados, la extracci√≥n de datos de cada p√°gina y la navegaci√≥n din√°mica entre p√°ginas mediante enlaces \"Siguiente\". Adem√°s, incorpora validaciones y mensajes de control para detectar errores cr√≠ticos durante el proceso de scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b82f6-a581-4db0-8d8a-4b8cfc40db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. ORQUESTACI√ìN Y PAGINACI√ìN DIN√ÅMICA ---\n",
    "def ejecutar_scraper_amazon_ofertas(url_inicial, num_paginas):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso completo de scraping de ofertas desde Amazon,\n",
    "    gestionando la paginaci√≥n din√°mica y centralizando la l√≥gica de\n",
    "    descarga, extracci√≥n y acumulaci√≥n de los datos recolectados.\n",
    "\n",
    "    Par√°metros:\n",
    "    url_inicial (str): URL inicial de b√∫squeda en Amazon.\n",
    "    num_paginas (int): N√∫mero m√°ximo de p√°ginas a recorrer.\n",
    "\n",
    "    Retorna:\n",
    "    list: Lista consolidada de diccionarios con los datos de todas\n",
    "    las ofertas v√°lidas encontradas durante la ejecuci√≥n.\n",
    "    \"\"\"\n",
    "    total_datos = []\n",
    "    url_actual = url_inicial\n",
    "    \n",
    "    # Bucle principal de paginaci√≥n\n",
    "    for pagina_actual in range(1, num_paginas + 1):\n",
    "        if url_actual is None:\n",
    "            print(\"‚ö†Ô∏è No se encontr√≥ el enlace a la p√°gina siguiente. Finalizando.\")\n",
    "            break\n",
    "            \n",
    "        # Mensaje informativo de progreso\n",
    "        print(f\"\\nüì¢ Procesando p√°gina {pagina_actual}/{num_paginas}. URL actual: {url_actual}\")\n",
    "        \n",
    "        # Obtenci√≥n y parseo del contenido HTML\n",
    "        soup = obtener_contenido_pagina(url_actual)\n",
    "        \n",
    "        if soup is None:\n",
    "            print(\"üõë Error al obtener la p√°gina. Deteniendo el scraper.\")\n",
    "            break\n",
    "            \n",
    "        # Extracci√≥n de datos de productos en oferta\n",
    "        nuevos_datos = extraer_datos_amazon_ofertas(soup) \n",
    "        \n",
    "        # Validaci√≥n cr√≠tica en la primera p√°gina\n",
    "        if not nuevos_datos and pagina_actual == 1:\n",
    "            print(\"‚ö†Ô∏è ¬°FALLO CR√çTICO! No se encontraron productos con oferta en la p√°gina 1.\")\n",
    "            break\n",
    "        \n",
    "        # Acumulaci√≥n de los datos extra√≠dos\n",
    "        total_datos.extend(nuevos_datos)\n",
    "        \n",
    "        # B√∫squeda del enlace a la siguiente p√°gina\n",
    "        enlace_siguiente = soup.find('a', class_='s-pagination-next')\n",
    "\n",
    "        if enlace_siguiente:\n",
    "            url_actual = \"https://www.amazon.com\" + enlace_siguiente.get('href')\n",
    "        else:\n",
    "            url_actual = None \n",
    "\n",
    "        # Delay para respetar los servidores y evitar bloqueos\n",
    "        time.sleep(3) \n",
    "\n",
    "    return total_datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6436ec7-8d89-495c-84e8-359c1e2338c1",
   "metadata": {},
   "source": [
    "### EJECUCI√ìN FINAL DEL SCRAPER Y PRESENTACI√ìN DE RESULTADOS\n",
    "Esta secci√≥n ejecuta el scraper completo cuando el archivo se ejecuta como programa principal. Adem√°s, se encarga de:\n",
    "- Mostrar mensajes de estado del proceso\n",
    "- Imprimir una muestra de los datos obtenidos\n",
    "- Convertir los resultados a un DataFrame\n",
    "- Guardar la informaci√≥n final en un archivo CSV para an√°lisis posterior o trabajo colaborativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc0eda-c5cd-4a0e-a445-bd202e03ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. EJECUCI√ìN E IMPRESI√ìN COMO LISTA DE DICCIONARIOS ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Ejecutar el proceso de extracci√≥n\n",
    "    resultados_finales_diccionario = ejecutar_scraper_amazon_ofertas(URL_INICIAL, NUMERO_DE_PAGINAS)\n",
    "    \n",
    "    # ... (c√≥digo de impresi√≥n y muestra) ...\n",
    "    \n",
    "    # Verificar si se obtuvieron resultados v√°lidos\n",
    "    if resultados_finales_diccionario:\n",
    "        print(f\"\\n‚úÖ Extracci√≥n de Amazon completada. Total de productos filtrados: {len(resultados_finales_diccionario)}.\")\n",
    "        \n",
    "        # Imprimir la salida final en formato de lista de diccionarios\n",
    "        print(\"\\n--- SALIDA FINAL: LISTA DE DICCIONARIOS (Muestra de Ofertas Reales) ---\")\n",
    "        \n",
    "        if len(resultados_finales_diccionario) > 0:\n",
    "            # Imprimir solo una muestra para evitar saturar la consola\n",
    "            print(json.dumps(resultados_finales_diccionario[:5], indent=4))\n",
    "            print(f\"\\n... Se omiten {len(resultados_finales_diccionario) - 5} productos m√°s para la vista previa. Total: {len(resultados_finales_diccionario)}.\")\n",
    "        \n",
    "        # Conversi√≥n de los resultados a DataFrame para an√°lisis\n",
    "        df = pd.DataFrame(resultados_finales_diccionario)\n",
    "        \n",
    "        # Definici√≥n del nombre del archivo CSV de salida\n",
    "        NOMBRE_ARCHIVO_CSV = 'amazon_ofertas_filtradas.csv'\n",
    "        \n",
    "        # Guardar los datos en formato CSV\n",
    "        df.to_csv(NOMBRE_ARCHIVO_CSV, index=False, encoding='utf-8')\n",
    "        print(f\"\\nüíæ Datos guardados en CSV: {NOMBRE_ARCHIVO_CSV}\")\n",
    "        \n",
    "    else:\n",
    "        # Mensaje informativo cuando no se obtienen resultados\n",
    "        print(\"\\n‚ö†Ô∏è La extracci√≥n de Amazon no produjo resultados con los filtros aplicados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e5c29",
   "metadata": {},
   "source": [
    "### IMPORTACI√ìN DE LIBRER√çAS PARA WEB SCRAPING DIN√ÅMICO\n",
    "\n",
    "Este bloque importa todas las librer√≠as necesarias para realizar web scraping din√°mico utilizando Selenium.\n",
    "El objetivo es automatizar la navegaci√≥n web, esperar la carga din√°mica de contenido, extraer informaci√≥n estructurada y almacenarla para su posterior an√°lisis.\n",
    "\n",
    "\n",
    "Este c√≥digo ser√° utilizado para el scraping de p√°ginas como Coolbox y Falabella, que cargan contenido v√≠a JavaScript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4407a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.39.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (2.2.3)\n",
      "Collecting urllib3<3.0,>=2.5.0 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
      "  Using cached trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
      "  Using cached trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2025.10.5 (from selenium)\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting typing_extensions<5.0,>=4.15.0 (from selenium)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /opt/anaconda3/lib/python3.13/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (3.7)\n",
      "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
      "  Using cached wsproto-1.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.13/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.13/site-packages (from webdriver-manager) (1.1.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.13/site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in /opt/anaconda3/lib/python3.13/site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Using cached selenium-4.39.0-py3-none-any.whl (9.7 MB)\n",
      "Using cached trio-0.32.0-py3-none-any.whl (512 kB)\n",
      "Using cached trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached wsproto-1.3.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wsproto, urllib3, typing_extensions, outcome, certifi, trio, webdriver-manager, trio-websocket, selenium\n",
      "\u001b[2K  Attempting uninstall: urllib3‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0/9\u001b[0m [wsproto]\n",
      "\u001b[2K    Found existing installation: urllib3 2.3.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/9\u001b[0m [urllib3]\n",
      "\u001b[2K    Uninstalling urllib3-2.3.0:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/9\u001b[0m [urllib3]\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.3.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/9\u001b[0m [urllib3]\n",
      "\u001b[2K  Attempting uninstall: typing_extensions‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/9\u001b[0m [urllib3]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.12.2‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/9\u001b[0m [typing_extensions]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.12.2:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/9\u001b[0m [typing_extensions]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.12.2‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/9\u001b[0m [typing_extensions]\n",
      "\u001b[2K  Attempting uninstall: certifi\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/9\u001b[0m [outcome]xtensions]\n",
      "\u001b[2K    Found existing installation: certifi 2025.4.26‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/9\u001b[0m [certifi]\n",
      "\u001b[2K    Uninstalling certifi-2025.4.26:\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/9\u001b[0m [certifi]\n",
      "\u001b[2K      Successfully uninstalled certifi-2025.4.26‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/9\u001b[0m [certifi]\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9/9\u001b[0m [selenium]8/9\u001b[0m [selenium]-manager]\n",
      "\u001b[1A\u001b[2KSuccessfully installed certifi-2025.11.12 outcome-1.3.0.post0 selenium-4.39.0 trio-0.32.0 trio-websocket-0.12.2 typing_extensions-4.15.0 urllib3-2.6.2 webdriver-manager-4.0.2 wsproto-1.3.2\n"
     ]
    }
   ],
   "source": [
    "#Necesario para usar selenium webdriver y pandas\n",
    "!pip install selenium webdriver-manager pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa63d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time                    \n",
    "import json                   \n",
    "import pandas as pd           \n",
    "import os                      # Operaciones con el sistema de archivos\n",
    "import re                      # Expresiones regulares para limpieza de texto\n",
    "\n",
    "# Librer√≠as principales de Selenium para automatizaci√≥n web\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Herramientas para esperas expl√≠citas (carga din√°mica de elementos)\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Gestor autom√°tico del ChromeDriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a95b70",
   "metadata": {},
   "source": [
    "### CONFIGURACI√ìN INCIAL PARA SCRAPING DIN√ÅMICO\n",
    "En esta secci√≥n se definen los par√°metros generales que ser√°n utilizados durante el scraping din√°mico mediante Selenium.\n",
    "\n",
    "Incluye el t√©rmino de busqueda y los tiempos de espera necesarios para interactuar con las p√°ginas web din√°micas como Coolbox y Falabella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0370874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURACI√ìN INICIAL PARA SCRAPING DIN√ÅMICO ---\n",
    "\n",
    "TERMINO_BUSQUEDA = \"laptop\"   # Producto a buscar en las tiendas\n",
    "TIEMPO_ESPERA = 20            # Tiempo m√°ximo de espera expl√≠cita (segundos)\n",
    "\n",
    "# --- FUNCI√ìN AUXILIAR PARA LIMPIEZA Y NORMALIZACI√ìN DE PRECIOS ---\n",
    "# Esta funci√≥n permite transformar los precios extra√≠dos desde\n",
    "# p√°ginas web din√°micas (en formato texto) a un formato num√©rico\n",
    "# est√°ndar, eliminando s√≠mbolos monetarios y caracteres\n",
    "# innecesarios, con el fin de permitir c√°lculos de descuentos\n",
    "# y comparaciones de precios.\n",
    "\n",
    "def limpiar_texto_precio(texto_sucio):\n",
    "    if not isinstance(texto_sucio, str):\n",
    "        return None, \"N/A\"\n",
    "    \n",
    "    # 1. Quitar saltos de l√≠nea y espacios innecesarios\n",
    "    texto_plano = texto_sucio.replace('\\n', '').replace('\\r', '').replace('\\t', '').strip()\n",
    "    \n",
    "    # 2. Eliminar s√≠mbolos de moneda y caracteres no num√©ricos\n",
    "    # (Se conservan √∫nicamente d√≠gitos y puntos decimales)\n",
    "    solo_numeros = re.sub(r'[^\\d.]', '', texto_plano.replace(',', '')) \n",
    "    \n",
    "    valor_float = None\n",
    "    try:\n",
    "        valor_float = float(solo_numeros)\n",
    "    except ValueError:\n",
    "        pass\n",
    "        \n",
    "    # Retorna el valor num√©rico (para c√°lculos) y el texto limpio (para visualizaci√≥n)\n",
    "    return valor_float, texto_plano\n",
    "\n",
    "\n",
    "# --- FUNCI√ìN DE INICIALIZACI√ìN DEL NAVEGADOR (SELENIUM) ---\n",
    "# ==========================================================\n",
    "# Esta funci√≥n configura e inicia el navegador Google Chrome\n",
    "# utilizando Selenium, simulando un navegador real mediante\n",
    "# el uso de User-Agent y deshabilitando ciertos indicadores\n",
    "# de automatizaci√≥n.\n",
    "# ==========================================================\n",
    "def iniciar_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--start-maximized')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# --- FUNCI√ìN AUXILIAR PARA CIERRE AUTOM√ÅTICO DE POPUPS ---\n",
    "# ==========================================================\n",
    "# Durante el scraping din√°mico, muchas p√°ginas presentan\n",
    "# ventanas emergentes (cookies, anuncios o promociones)\n",
    "# que bloquean la interacci√≥n. Esta funci√≥n intenta\n",
    "# cerrarlas autom√°ticamente utilizando selectores comunes.\n",
    "# ==========================================================\n",
    "def intentar_cerrar_popups(driver):\n",
    "    print(\"    üßπ Intentando cerrar popups...\")\n",
    "    \n",
    "    selectores = [\n",
    "        \"button#onetrust-accept-btn-handler\",\n",
    "        \"div.crs-close\",\n",
    "        \"div#cookies-consent button\",\n",
    "        \"button[class*='closeButton']\",\n",
    "        \"div[class*='modal'] button\",\n",
    "        \"div#dy-modal-contents button.close\",  # Popup t√≠pico de Falabella\n",
    "        \"span[class*='close-icon']\"\n",
    "    ]\n",
    "    \n",
    "    for sel in selectores:\n",
    "        try:\n",
    "            btns = driver.find_elements(By.CSS_SELECTOR, sel)\n",
    "            for btn in btns:\n",
    "                if btn.is_displayed():\n",
    "                    driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                    time.sleep(0.5)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# --- FUNCI√ìN AUXILIAR PARA B√öSQUEDA SEGURA DE TEXTO ---\n",
    "# ==========================================================\n",
    "# Esta funci√≥n intenta extraer texto desde un elemento web\n",
    "# probando m√∫ltiples selectores CSS, lo que permite manejar\n",
    "# variaciones en la estructura HTML de los productos.\n",
    "# ==========================================================\n",
    "def buscar_texto(elemento, selectores):\n",
    "    for sel in selectores:\n",
    "        try:\n",
    "            etiqueta = elemento.find_element(By.CSS_SELECTOR, sel)\n",
    "            texto = etiqueta.text.strip()\n",
    "            if texto:\n",
    "                return texto\n",
    "        except:\n",
    "            continue\n",
    "    return \"N/A\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42beb09",
   "metadata": {},
   "source": [
    "### FUNCI√ìN DE INICIALIZACI√ìN DEL NAVEGADOR (SELENIUM)\n",
    "\n",
    "Esta funci√≥n configura e inicia el navegador Google Chrome utilizando Selenium, simulando un navegador real mediante el uso de User-Agent y deshabilitando ciertos indicadores de automatizaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ebedac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iniciar_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--start-maximized')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "\n",
    "# --- FUNCI√ìN AUXILIAR PARA CIERRE AUTOM√ÅTICO DE POPUPS ---\n",
    "# Durante el scraping din√°mico, muchas p√°ginas presentan\n",
    "# ventanas emergentes (cookies, anuncios o promociones)\n",
    "# que bloquean la interacci√≥n. Esta funci√≥n intenta\n",
    "# cerrarlas autom√°ticamente utilizando selectores comunes.\n",
    "\n",
    "def intentar_cerrar_popups(driver):\n",
    "    print(\"    üßπ Intentando cerrar popups...\")\n",
    "    \n",
    "    selectores = [\n",
    "        \"button#onetrust-accept-btn-handler\",\n",
    "        \"div.crs-close\",\n",
    "        \"div#cookies-consent button\",\n",
    "        \"button[class*='closeButton']\",\n",
    "        \"div[class*='modal'] button\",\n",
    "        \"div#dy-modal-contents button.close\",  # Popup t√≠pico de Falabella\n",
    "        \"span[class*='close-icon']\"\n",
    "    ]\n",
    "    \n",
    "    for sel in selectores:\n",
    "        try:\n",
    "            btns = driver.find_elements(By.CSS_SELECTOR, sel)\n",
    "            for btn in btns:\n",
    "                if btn.is_displayed():\n",
    "                    driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                    time.sleep(0.5)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "# --- FUNCI√ìN AUXILIAR PARA B√öSQUEDA SEGURA DE TEXTO ---\n",
    "# Esta funci√≥n intenta extraer texto desde un elemento web\n",
    "# probando m√∫ltiples selectores CSS, lo que permite manejar\n",
    "# variaciones en la estructura HTML de los productos.\n",
    "\n",
    "def buscar_texto(elemento, selectores):\n",
    "    for sel in selectores:\n",
    "        try:\n",
    "            etiqueta = elemento.find_element(By.CSS_SELECTOR, sel)\n",
    "            texto = etiqueta.text.strip()\n",
    "            if texto:\n",
    "                return texto\n",
    "        except:\n",
    "            continue\n",
    "    return \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5146e74d",
   "metadata": {},
   "source": [
    "### L√ìGICA DE EXTRACCI√ìN DE PRODUCTOS\n",
    "Esta funci√≥n centraliza la l√≥gica de extracci√≥n de productos desde tiendas con contenido din√°mico utilizando Selenium.\n",
    "\n",
    "Permite procesar m√∫ltiples tiendas (Coolbox, Falabella), manejando scroll din√°mico, detecci√≥n de estructuras HTML variables, limpieza de precios y c√°lculo de descuentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a388e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_tienda(driver, nombre_tienda, url):\n",
    "    datos = []\n",
    "    \n",
    "    print(f\"\\nüì¢ Procesando {nombre_tienda}... URL: {url}\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Espera inicial para permitir la carga completa del contenido din√°mico\n",
    "    time.sleep(6) \n",
    "    \n",
    "    # Intentar cerrar ventanas emergentes que bloqueen la interacci√≥n\n",
    "    intentar_cerrar_popups(driver)\n",
    "    \n",
    "    # SCROLL AUTOM√ÅTICO PARA ACTIVAR CARGA DE PRODUCTOS\n",
    "    print(\"    ‚¨áÔ∏è Bajando (Scroll) para activar carga...\")\n",
    "    for i in range(5): \n",
    "        driver.execute_script(f\"window.scrollTo(0, {(i+1)*800});\")\n",
    "        time.sleep(1.5)\n",
    "\n",
    "    # DETECCI√ìN DE CONTENEDORES DE PRODUCTOS\n",
    "    # Se prueban m√∫ltiples selectores CSS para soportar\n",
    "    # distintas estructuras HTML utilizadas por Falabella\n",
    "    # y Coolbox (incluyendo versiones modernas y cl√°sicas).\n",
    "    selectores_contenedor = [\n",
    "        \"div[id^='testId-pod-display']\",            # Falabella (ID espec√≠fico)\n",
    "        \"div.pod-item\",                             # Falabella (Clase gen√©rica)\n",
    "        \"div.vtex-product-summary-2-x-container\",   # Coolbox Moderno\n",
    "        \"div.product-item\",                         # Coolbox Cl√°sico\n",
    "        \"div.Showcase__item\",                       # Otros layouts VTEX\n",
    "        \"div[class*='galleryItem']\",\n",
    "    ]\n",
    "    \n",
    "    productos = []\n",
    "    for selector in selectores_contenedor:\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "        if len(elems) > 0:\n",
    "            print(f\"    ‚úÖ Estructura detectada: '{selector}' ({len(elems)} items)\")\n",
    "            productos = elems\n",
    "            break\n",
    "            \n",
    "    # PLAN B: B√öSQUEDA MEDIANTE XPATH\n",
    "    # Si no se detectan productos mediante selectores CSS,\n",
    "    # se utiliza una estrategia alternativa basada en XPath.\n",
    "    if not productos:\n",
    "        try:\n",
    "            productos = driver.find_elements(\n",
    "                By.XPATH,\n",
    "                \"//div[contains(., 'S/') and string-length(.) < 400 and count(descendant::img)=1]\"\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if not productos:\n",
    "        print(f\"‚ùå No se encontraron productos en {nombre_tienda}.\")\n",
    "        return []\n",
    "\n",
    "    # EXTRACCI√ìN Y AN√ÅLISIS DE PRODUCTOS\n",
    "    print(f\"    ‚öôÔ∏è Analizando precios de los primeros 15 productos...\")\n",
    "    contador = 0\n",
    "    \n",
    "    for item in productos:\n",
    "        if contador >= 15:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # 1. NOMBRE DEL PRODUCTO\n",
    "            nombre = buscar_texto(item, [\n",
    "                \"b[id^='testId-pod-display-product-title']\",  # Falabella T√≠tulo\n",
    "                \"b[class*='pod-subTitle']\",                   # Falabella Subt√≠tulo\n",
    "                \"span[class*='productBrand']\",                # Coolbox\n",
    "                \"h3\",\n",
    "                \".product-item-link\", \n",
    "                \"div[class*='name']\"\n",
    "            ])\n",
    "            \n",
    "            # 2. PRECIOS (TEXTO CRUDO)\n",
    "            precio_actual_raw = buscar_texto(item, [\n",
    "                \"span[id^='testId-pod-display-price']\",       # Falabella Precio\n",
    "                \"div[class*='sellingPrice']\",                 # Coolbox\n",
    "                \"span[class*='sellingPrice']\",\n",
    "                \".price\",\n",
    "                \".Showcase__salePrice\"\n",
    "            ])\n",
    "            \n",
    "            # Precio anterior (tachado o lista)\n",
    "            precio_antes_raw = buscar_texto(item, [\n",
    "                \"span[class*='copy10']\",                      # Falabella Tachado\n",
    "                \"ol li[class*='price-old']\",                  # Falabella Lista antigua\n",
    "                \"div[class*='listPrice']\",                    # Coolbox\n",
    "                \"span[class*='listPrice']\", \n",
    "                \".old-price\", \n",
    "                \"span[style*='line-through']\"\n",
    "            ])\n",
    "            \n",
    "            # 3. LIMPIEZA Y NORMALIZACI√ìN DE PRECIOS\n",
    "            val_actual, txt_actual_limpio = limpiar_texto_precio(precio_actual_raw)\n",
    "            val_antes, txt_antes_limpio = limpiar_texto_precio(precio_antes_raw)\n",
    "            \n",
    "            # PARCHE ESPEC√çFICO PARA FALABELLA\n",
    "            # En algunos casos, Falabella presenta m√∫ltiples precios\n",
    "            # en un solo bloque de texto. Se extraen y ordenan\n",
    "            # para inferir precio actual y precio anterior.\n",
    "            if val_actual is None:\n",
    "                numeros = re.findall(r'S/\\s*[\\d,]+(?:\\.\\d+)?', item.text)\n",
    "                if numeros:\n",
    "                    precios_limpios = []\n",
    "                    for n in numeros:\n",
    "                        v, t = limpiar_texto_precio(n)\n",
    "                        if v:\n",
    "                            precios_limpios.append(v)\n",
    "                    \n",
    "                    if precios_limpios:\n",
    "                        precios_limpios.sort(reverse=True)\n",
    "                        if len(precios_limpios) > 1:\n",
    "                            val_antes = precios_limpios[0]\n",
    "                            val_actual = precios_limpios[-1]\n",
    "                            txt_antes_limpio = f\"S/ {val_antes:,.2f}\"\n",
    "                            txt_actual_limpio = f\"S/ {val_actual:,.2f}\"\n",
    "                        else:\n",
    "                            val_actual = precios_limpios[0]\n",
    "                            txt_actual_limpio = f\"S/ {val_actual:,.2f}\"\n",
    "\n",
    "            # Si no existe precio anterior, se asume igual al actual\n",
    "            if val_antes is None:\n",
    "                val_antes = val_actual\n",
    "                txt_antes_limpio = txt_actual_limpio\n",
    "\n",
    "            # 4. C√ÅLCULO DEL DESCUENTO\n",
    "            descuento = \"0%\"\n",
    "            if val_antes and val_actual and val_antes > val_actual:\n",
    "                diff = val_antes - val_actual\n",
    "                porc = (diff / val_antes) * 100\n",
    "                descuento = f\"{porc:.0f}%\"\n",
    "\n",
    "            # 5. URL DEL PRODUCTO E IMAGEN\n",
    "            try:\n",
    "                link = item.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "            except:\n",
    "                link = \"N/A\"\n",
    "                \n",
    "            try:\n",
    "                img = item.find_element(By.TAG_NAME, \"img\").get_attribute(\"src\")\n",
    "            except:\n",
    "                img = \"N/A\"\n",
    "\n",
    "            # FILTRO FINAL Y ALMACENAMIENTO\n",
    "            if nombre != \"N/A\" and val_actual is not None:\n",
    "                datos.append({\n",
    "                    \"nombre\": nombre.replace('\\n', ' '), \n",
    "                    \"precio_antes\": txt_antes_limpio,    \n",
    "                    \"precio_despues\": txt_actual_limpio, \n",
    "                    \"descuento\": descuento,\n",
    "                    \"url_image\": img,\n",
    "                    \"tienda\": nombre_tienda,\n",
    "                    \"url\": link\n",
    "                })\n",
    "                contador += 1\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "    return datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed75ba1a",
   "metadata": {},
   "source": [
    "### ORQUESTACI√ìN FINAL Y ALMACENAMIENTO DE RESULTADOS\n",
    "Este bloque ejecuta el flujo completo del scraping din√°mico:\n",
    "1. Inicializa el navegador automatizado (Selenium).\n",
    "2. Ejecuta la extracci√≥n de productos desde m√∫ltiples tiendas (Coolbox y Falabella).\n",
    "3. Consolida la informaci√≥n extra√≠da en una sola estructura.\n",
    "4. Exporta los resultados en formatos CSV y JSON para an√°lisis posterior y trabajo colaborativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b561fea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¢ Procesando Coolbox... URL: https://www.coolbox.pe/laptop\n",
      "    üßπ Intentando cerrar popups...\n",
      "    ‚¨áÔ∏è Bajando (Scroll) para activar carga...\n",
      "    ‚úÖ Estructura detectada: 'div[class*='galleryItem']' (24 items)\n",
      "    ‚öôÔ∏è Analizando precios de los primeros 15 productos...\n",
      "\n",
      "üì¢ Procesando Falabella... URL: https://www.falabella.com.pe/falabella-pe/category/cat40712/Laptops\n",
      "    üßπ Intentando cerrar popups...\n",
      "    ‚¨áÔ∏è Bajando (Scroll) para activar carga...\n",
      "    ‚öôÔ∏è Analizando precios de los primeros 15 productos...\n",
      "\n",
      "‚úÖ √âXITO TOTAL: 30 productos extra√≠dos.\n",
      "üíæ Datos guardados y LIMPIOS en carpeta 'data/'\n",
      "   nombre precio_antes precio_despues descuento  \\\n",
      "0   APPLE     S/ 2,999       S/ 2,799        7%   \n",
      "1   APPLE     S/ 4,599       S/ 3,599       22%   \n",
      "2   APPLE     S/ 5,299       S/ 3,779       29%   \n",
      "3    ASUS     S/ 3,799       S/ 3,499        8%   \n",
      "4  LENOVO     S/ 2,599       S/ 1,812       30%   \n",
      "\n",
      "                                           url_image   tienda  \\\n",
      "0  https://coolboxpe.vtexassets.com/arquivos/ids/...  Coolbox   \n",
      "1  https://coolboxpe.vtexassets.com/arquivos/ids/...  Coolbox   \n",
      "2  https://coolboxpe.vtexassets.com/arquivos/ids/...  Coolbox   \n",
      "3  https://coolboxpe.vtexassets.com/arquivos/ids/...  Coolbox   \n",
      "4  https://coolboxpe.vtexassets.com/arquivos/ids/...  Coolbox   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.coolbox.pe/laptop-apple-macbook-ai...  \n",
      "1  https://www.coolbox.pe/macbook-air-13-6-chip-m...  \n",
      "2  https://www.coolbox.pe/macbook-air-13--256gb-1...  \n",
      "3  https://www.coolbox.pe/asus-rog-xbox-ally-x-20...  \n",
      "4  https://www.coolbox.pe/laptop-hp-15fc0012la-am...  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Inicializaci√≥n del driver de Selenium\n",
    "    driver = iniciar_driver()\n",
    "    \n",
    "    # Estructura para almacenar todos los productos extra√≠dos\n",
    "    all_data = []\n",
    "    \n",
    "    # 1. EXTRACCI√ìN DESDE COOLBOX\n",
    "    # Se utiliza la b√∫squeda directa por t√©rmino, manteniendo\n",
    "    # la l√≥gica previamente definida para scraping din√°mico.\n",
    "    all_data.extend(\n",
    "        extraer_tienda(\n",
    "            driver,\n",
    "            \"Coolbox\",\n",
    "            f\"https://www.coolbox.pe/{TERMINO_BUSQUEDA}\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 2. EXTRACCI√ìN DESDE FALABELLA\n",
    "    # Se emplea una URL de categor√≠a directa, la cual resulta\n",
    "    # m√°s estable que las b√∫squedas internas del sitio.\n",
    "    url_falabella = \"https://www.falabella.com.pe/falabella-pe/category/cat40712/Laptops\"\n",
    "    all_data.extend(\n",
    "        extraer_tienda(\n",
    "            driver,\n",
    "            \"Falabella\",\n",
    "            url_falabella\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Cierre controlado del navegador\n",
    "    driver.quit()\n",
    "    \n",
    "    # VALIDACI√ìN Y EXPORTACI√ìN DE RESULTADOS\n",
    "    if all_data:\n",
    "        print(f\"\\n‚úÖ √âXITO TOTAL: {len(all_data)} productos extra√≠dos.\")\n",
    "        \n",
    "        # Crear carpeta de salida si no existe\n",
    "        if not os.path.exists('data'):\n",
    "            os.makedirs('data')\n",
    "        \n",
    "        # Conversi√≥n a DataFrame para an√°lisis estructurado\n",
    "        df = pd.DataFrame(all_data)\n",
    "        \n",
    "        # Exportaci√≥n a CSV (formato principal para an√°lisis)\n",
    "        df.to_csv(\n",
    "            'data/dinamico_ofertas_filtradas.csv',\n",
    "            index=False,\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        \n",
    "        # Exportaci√≥n a JSON \n",
    "        with open('data/dinamico_ofertas.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        print(\"üíæ Datos guardados y LIMPIOS en carpeta 'data/'\")\n",
    "        \n",
    "        # Vista previa de los primeros registros\n",
    "        print(df.head())\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå Error. Revisa las capturas de pantalla.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
