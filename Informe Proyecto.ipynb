{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d90d895-dde4-4738-a100-9ccab57d5f79",
   "metadata": {},
   "source": [
    "# Trabajo Final - Lenguaje de Programaci√≥n 2\n",
    "### T√≠tulo del Proyecto\n",
    "**Sistema de Monitoreo y Comparaci√≥n de Precios de Hardware (Smarth Shop)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b3c35-fbdd-450d-8247-6ada3407fb8a",
   "metadata": {},
   "source": [
    "### Nota\n",
    "\n",
    "Todo el desarrollo del proyecto se ha realizado respetando las directivas del archivo robots.txt y los t√©rminos de servicio de las p√°ginas web y APIs consultadas. Asimismo, se implementaron tiempos de espera (delays) entre las solicitudes para evitar la sobrecarga de los servidores externos y asegurar una extracci√≥n responsable de la informaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214f854-e438-4e8e-bb30-198c146dbaf6",
   "metadata": {},
   "source": [
    "### Introducci√≥n y Relevancia del Proyecto\n",
    "\n",
    "En un contexto econ√≥mico marcado por la inflaci√≥n y la constante variaci√≥n de precios, los consumidores peruanos enfrentan dificultades para identificar ofertas reales en productos tecnol√≥gicos como laptops y componentes de hardware. Muchas veces, las promociones mostradas en tiendas virtuales no reflejan un verdadero ahorro, lo que genera desinformaci√≥n y decisiones de compra poco √≥ptimas.\n",
    "\n",
    "El proyecto **Smart Shop** surge como una soluci√≥n tecnol√≥gica orientada a centralizar, comparar y normalizar precios de productos de hardware provenientes de distintas tiendas, permitiendo a los usuarios identificar el precio m√°s bajo disponible en el mercado al momento de la consulta. De esta manera, el sistema contribuye al ahorro econ√≥mico, fomenta decisiones de compra informadas y protege el poder adquisitivo de personas que dependen de la tecnolog√≠a para estudiar, trabajar o emprender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0466eba7-821b-46a1-ba01-3b0a094b0ca2",
   "metadata": {},
   "source": [
    "### Objetivos del Proyecto\n",
    "#### Objetivo General\n",
    "\n",
    "Desarrollar un sistema automatizado en Python que permita monitorear y comparar precios de productos tecnol√≥gicos entre diferentes retailers, presentando la informaci√≥n de forma estructurada y estandarizada.\n",
    "\n",
    "#### Objetivos Espec√≠ficos\n",
    "\n",
    "- Desarrollar un bot en Python capaz de extraer diariamente precios de laptops y componentes de hardware desde tiendas reconocidas.\n",
    "- Implementar la normalizaci√≥n de moneda (USD a PEN) en tiempo real mediante el consumo de una API de tipo de cambio.\n",
    "- Generar un dataset estructurado en formato CSV que permita analizar y visualizar la dispersi√≥n de precios de un mismo producto.\n",
    "- Facilitar la identificaci√≥n del producto m√°s econ√≥mico disponible en la fecha de ejecuci√≥n del sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b67fd01-9a33-43b2-afe8-8525e38ec77a",
   "metadata": {},
   "source": [
    "### Fuentes de Datos y Estrategia de Extracci√≥n\n",
    "\n",
    "El proyecto utiliza **tres tipos de fuentes de datos**, cumpliendo con los requisitos del curso y aplicando distintas t√©cnicas de adquisici√≥n de informaci√≥n:\n",
    "\n",
    "#### Fuente 1: Web Scraping Est√°tico\n",
    "- **Amazon**\n",
    "- Se emplea la librer√≠a **BeautifulSoup** para extraer informaci√≥n de cat√°logos de laptops.\n",
    "- Los datos recolectados incluyen: nombre del producto, precio, descripci√≥n y vendedor.\n",
    "- Esta fuente permite obtener informaci√≥n estructurada directamente desde el HTML est√°tico.\n",
    "\n",
    "#### Fuente 2: Web Scraping Din√°mico\n",
    "- **Plaza Vea, Coolbox y Falabella**\n",
    "- Se utiliza **Selenium** para renderizar contenido din√°mico generado mediante JavaScript.\n",
    "- Se extraen precios y ofertas exclusivas disponibles √∫nicamente en la web.\n",
    "- Esta t√©cnica permite acceder a informaci√≥n que no est√° disponible mediante scraping tradicional.\n",
    "\n",
    "#### Fuente 3: API P√∫blica\n",
    "- **ExchangeRate-API**\n",
    "- Se consume una API REST para obtener el tipo de cambio actualizado entre d√≥lares estadounidenses (USD) y soles peruanos (PEN).\n",
    "- Esta informaci√≥n es clave para estandarizar todos los precios recolectados y permitir una comparaci√≥n precisa entre tiendas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631d3e2-4b77-4ed1-af33-e16b35122030",
   "metadata": {},
   "source": [
    "### Integraci√≥n y Tratamiento de Datos\n",
    "\n",
    "Una vez obtenidos los datos desde las distintas fuentes, el sistema realiza un proceso de integraci√≥n y limpieza que incluye:\n",
    "- Conversi√≥n de todos los precios a moneda local (PEN).\n",
    "- Normalizaci√≥n de nombres de productos para facilitar la comparaci√≥n.\n",
    "- Eliminaci√≥n de registros duplicados.\n",
    "- Manejo de valores nulos o inconsistentes.\n",
    "\n",
    "El resultado de este proceso es un dataset limpio y estructurado, listo para su an√°lisis posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fda0ba-bc63-4d86-8a65-dc912f1771b3",
   "metadata": {},
   "source": [
    "### Producto Final\n",
    "\n",
    "El producto final del proyecto consiste en:\n",
    "- Un **repositorio en GitHub** que contiene todo el c√≥digo fuente, documentado y organizado por m√≥dulos.\n",
    "- Un **archivo CSV/Excel** que presenta la comparaci√≥n de precios de los productos analizados.\n",
    "- Un reporte que permite identificar claramente cu√°l es el producto m√°s barato del mercado en la fecha de ejecuci√≥n del sistema.\n",
    "\n",
    "Este sistema puede servir como base para futuras extensiones, como visualizaciones interactivas, alertas de precios o una aplicaci√≥n web orientada al consumidor final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63764d-cd8c-43c7-85ab-8a74da346871",
   "metadata": {},
   "source": [
    "### Tecnolog√≠as Utilizadas\n",
    "- **Lenguaje:** Python 3.x\n",
    "- **Librer√≠as principales:**\n",
    "    - Requests\n",
    "    - BeautifulSoup4\n",
    "    - Selenium\n",
    "    - Pandas\n",
    "\n",
    "- **Formato de salida:** CSV / Excel\n",
    "- **Control de versiones:** Git y GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d607b0-0080-4cc8-8bbe-0be7b728cd3a",
   "metadata": {},
   "source": [
    "### Importaci√≥n de librer√≠as necesarias para la extracci√≥n, procesamiento y almacenamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585bc11-6a7c-4dc0-910c-634f388ec937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19fc21a-0af9-4a0e-9d78-1ee2e8b29f08",
   "metadata": {},
   "source": [
    "### Configuraci√≥n de par√°metros para Amazon y definici√≥n de funci√≥n de limpieza de precios\n",
    "En esta secci√≥n se definen los par√°metros base para realizar el scraping de productos desde Amazon, incluyendo el t√©rmino de b√∫squeda, la cantidad de p√°ginas a recorrer y las cabeceras HTTP necesarias para simular un navegador real. Asimismo, se implementa una funci√≥n auxiliar para la limpieza y normalizaci√≥n de precios, la cual ser√° utilizada en etapas posteriores del proceso de extracci√≥n y an√°lisis de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38871b-a393-4636-8920-7ba5dcd86f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURACI√ìN INICIAL PARA AMAZON ---\n",
    "TERMINO_BUSQUEDA = \"computadores\"\n",
    "URL_INICIAL = f\"https://www.amazon.com/s?k={TERMINO_BUSQUEDA}\" \n",
    "NUMERO_DE_PAGINAS = 5  # Extracci√≥n de 5 p√°ginas\n",
    "\n",
    "\n",
    "# Headers avanzados para simular un navegador genuino y reducir\n",
    "# la probabilidad de bloqueos durante el scraping\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Referer': 'https://www.amazon.com/', \n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'es-ES,es;q=0.9',\n",
    "    'Cookie': 'custom_cookie=true',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "\n",
    "# --- FUNCI√ìN AUXILIAR PARA LIMPIEZA DE PRECIOS ---\n",
    "# Esta funci√≥n permite transformar los precios extra√≠dos desde\n",
    "# la web (en formato texto) a un formato num√©rico est√°ndar,\n",
    "# eliminando s√≠mbolos monetarios, separadores de miles y texto\n",
    "# irrelevante, con el fin de permitir c√°lculos y comparaciones.\n",
    "def limpiar_precio(precio_str):\n",
    "    \"\"\"\n",
    "    Limpia y normaliza el precio extra√≠do como texto, eliminando\n",
    "    s√≠mbolos monetarios, texto adicional y separadores innecesarios,\n",
    "    garantizando un formato num√©rico adecuado para an√°lisis posterior.\n",
    "\n",
    "    Par√°metros:\n",
    "    precio_str (str): Precio en formato texto obtenido durante el scraping.\n",
    "\n",
    "    Retorna:\n",
    "    float | None: Precio convertido a tipo float o None si la conversi√≥n falla.\n",
    "    \"\"\"\n",
    "    if isinstance(precio_str, str):\n",
    "        # 1. Eliminar caracteres no deseados y texto de referencia\n",
    "        precio_limpio = precio_str.replace('\\u00a0', ' ').replace('USD', '').replace('PEN', '').replace('S/', '').replace('$', '').replace('PVPR:', '').replace('Lista:', '').strip()\n",
    "        \n",
    "        # 2. Manejar separadores: eliminar comas utilizadas como separadores de miles\n",
    "        precio_limpio = precio_limpio.replace(',', '') \n",
    "        \n",
    "        # 3. Forzar formato de un solo punto decimal, eliminando puntos adicionales\n",
    "        if precio_limpio.count('.') > 1:\n",
    "            partes = precio_limpio.rsplit('.', 1) \n",
    "            entero = partes[0].replace('.', '') \n",
    "            decimal = partes[1] if len(partes) > 1 else '00'\n",
    "            precio_limpio = f\"{entero}.{decimal}\"\n",
    "        \n",
    "        # 4. Asegurar que no exista doble punto residual\n",
    "        precio_limpio = precio_limpio.replace('..', '.')\n",
    "        \n",
    "        try:\n",
    "            # 5. Conversi√≥n final a tipo float\n",
    "            return float(precio_limpio.strip())\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991a860-181a-44cd-9a7a-653332ac42d1",
   "metadata": {},
   "source": [
    "### FUNCI√ìN DE SOLICITUD HTTP Y PARSEO DEL CONTENIDO HTML\n",
    "Esta funci√≥n centraliza el proceso de conexi√≥n a la p√°gina web, realizando la solicitud HTTP y transformando la respuesta en un objeto BeautifulSoup. De esta manera, se desacopla la l√≥gica de descarga del contenido de la l√≥gica de extracci√≥n de datos, facilitando la reutilizaci√≥n del c√≥digo y el mantenimiento del sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6424fd-1b2b-46c2-ae9b-3220cdff7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. FUNCI√ìN DE SOLICITUD HTTP Y PARSEO ---\n",
    "def obtener_contenido_pagina(url):\n",
    "    \"\"\"\n",
    "    Realiza una solicitud HTTP GET a la URL indicada y devuelve el\n",
    "    contenido HTML parseado como un objeto BeautifulSoup.\n",
    "\n",
    "    Esta funci√≥n incorpora manejo de errores y validaci√≥n del c√≥digo\n",
    "    de estado HTTP, asegurando que solo se procese contenido v√°lido\n",
    "    durante la etapa de scraping.\n",
    "\n",
    "    Par√°metros:\n",
    "    url (str): Direcci√≥n web de la p√°gina a consultar.\n",
    "\n",
    "    Retorna:\n",
    "    BeautifulSoup | None: Objeto BeautifulSoup con el HTML parseado\n",
    "    o None si ocurre un error durante la solicitud.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        \n",
    "        # Mostrar el c√≥digo de estado HTTP para fines de monitoreo\n",
    "        print(f\"    - C√≥digo de estado HTTP recibido: {response.status_code}\") \n",
    "        \n",
    "        # Lanza una excepci√≥n si la respuesta HTTP indica error\n",
    "        response.raise_for_status() \n",
    "\n",
    "        # Parseo del contenido HTML con BeautifulSoup\n",
    "        return BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Manejo de errores de conexi√≥n, timeout o respuesta inv√°lida\n",
    "        print(f\"‚ùå Error al realizar la solicitud a {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10564be1-5ffe-4fa2-ac33-49276f4dedd8",
   "metadata": {},
   "source": [
    "### FUNCI√ìN DE EXTRACCI√ìN, FILTRADO Y PROCESAMIENTO DE OFERTAS\n",
    "Esta funci√≥n se encarga de recorrer el contenido HTML previamente parseado de Amazon y extraer informaci√≥n relevante √∫nicamente de productos que presentan una oferta real (precio anterior y precio actual). Se aplica un filtrado estricto para garantizar que los datos obtenidos sean consistentes y √∫tiles para el an√°lisis comparativo de precios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f1d397-1c86-463f-ae1e-93aee36108ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. FUNCI√ìN DE EXTRACCI√ìN Y FILTRADO ESTRICTO ---\n",
    "def extraer_datos_amazon_ofertas(soup):\n",
    "    \"\"\"\n",
    "    Extrae informaci√≥n de productos en oferta desde el HTML de Amazon,\n",
    "    aplicando un filtrado estricto para conservar √∫nicamente aquellos\n",
    "    productos que cuentan con nombre, precio anterior y precio actual.\n",
    "\n",
    "    La funci√≥n tambi√©n calcula el porcentaje de descuento utilizando\n",
    "    valores num√©ricos normalizados, garantizando consistencia para\n",
    "    an√°lisis posteriores.\n",
    "\n",
    "    Par√°metros:\n",
    "    soup (BeautifulSoup): Objeto BeautifulSoup con el HTML parseado\n",
    "    de la p√°gina de resultados de Amazon.\n",
    "\n",
    "    Retorna:\n",
    "    list: Lista de diccionarios con la informaci√≥n estructurada de\n",
    "    cada producto v√°lido encontrado.\n",
    "    \"\"\"\n",
    "    datos_productos = []\n",
    "\n",
    "    # Selector principal de contenedores de productos\n",
    "    contenedores_productos = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "    \n",
    "    # Mensaje informativo sobre la cantidad de productos encontrados\n",
    "    print(f\"    -> Productos encontrados para extraer: {len(contenedores_productos)}\")\n",
    "\n",
    "    for contenedor in contenedores_productos:\n",
    "        \n",
    "        # Inicializaci√≥n de variables con valores por defecto\n",
    "        nombre = 'N/A'\n",
    "        precio_antes = 'N/A'\n",
    "        precio_despues = 'N/A'\n",
    "        url_image = 'N/A'\n",
    "        \n",
    "        # --- Extracci√≥n del nombre del producto ---\n",
    "        titulo_h2 = contenedor.find('h2')\n",
    "        if titulo_h2:\n",
    "            span_titulo = titulo_h2.find('span')\n",
    "            nombre = span_titulo.text.strip() if span_titulo else titulo_h2.text.strip()\n",
    "        \n",
    "        # --- Extracci√≥n del precio actual (precio_despues) ---\n",
    "        precio_span = contenedor.find('span', class_='a-price')\n",
    "        if precio_span:\n",
    "            p_entero = precio_span.find('span', class_='a-price-whole')\n",
    "            p_fraccion = precio_span.find('span', class_='a-price-fraction')\n",
    "            moneda = precio_span.find('span', class_='a-price-symbol')\n",
    "            \n",
    "            p_entero_str = p_entero.text.strip() if p_entero else ''\n",
    "            p_fraccion_str = p_fraccion.text.strip() if p_fraccion else ''\n",
    "            moneda_str = moneda.text.strip() if moneda else 'USD'\n",
    "            \n",
    "            if p_entero_str or p_fraccion_str:\n",
    "                # Construcci√≥n del precio en formato texto limpio\n",
    "                precio_despues = f\"{moneda_str} {p_entero_str}.{p_fraccion_str}\".replace('\\xa0', ' ').replace('..', '.') \n",
    "\n",
    "        # --- Extracci√≥n del precio anterior (precio_antes) ---\n",
    "        precio_antes_tag = contenedor.find('span', class_='a-price', attrs={'data-a-strike': 'true'})\n",
    "        \n",
    "        if precio_antes_tag:\n",
    "            offscreen_price = precio_antes_tag.find('span', class_='a-offscreen')\n",
    "            if offscreen_price:\n",
    "                precio_antes = offscreen_price.text.strip().replace('\\u00a0', ' ')\n",
    "            else:\n",
    "                precio_antes = precio_antes_tag.text.strip().replace('\\u00a0', ' ')\n",
    "            \n",
    "            # Limpieza adicional del texto del precio anterior\n",
    "            precio_antes = precio_antes.replace('PVPR:', '').replace('Lista:', '').strip().replace('..', '.')\n",
    "        \n",
    "        # --- Extracci√≥n de la URL de la imagen del producto ---\n",
    "        imagen_tag = contenedor.find('img', class_='s-image')\n",
    "        url_image = imagen_tag.get('src') if imagen_tag and imagen_tag.get('src') else 'N/A'\n",
    "        \n",
    "        # --- C√°lculo del descuento ---\n",
    "        descuento = '0%'\n",
    "        \n",
    "        # Conversi√≥n de precios a formato num√©rico para el c√°lculo\n",
    "        num_despues = limpiar_precio(precio_despues)\n",
    "        num_antes = limpiar_precio(precio_antes) \n",
    "        \n",
    "        if num_antes and num_despues and num_antes > num_despues:\n",
    "            calc_descuento = ((num_antes - num_despues) / num_antes) * 100\n",
    "            descuento = f\"{calc_descuento:.0f}%\" \n",
    "        \n",
    "        # --- FILTRO ESTRICTO ---\n",
    "        # Solo se agregan productos con nombre, precio actual y precio anterior v√°lidos\n",
    "        if nombre != 'N/A' and precio_despues != 'N/A' and precio_antes != 'N/A':\n",
    "            datos_productos.append({\n",
    "                \"nombre\": nombre.replace('\\u00a0', ' '),\n",
    "                \"precio_antes\": precio_antes.replace('\\u00a0', ' '),\n",
    "                \"precio_despues\": precio_despues.replace('\\u00a0', ' '), \n",
    "                \"descuento\": descuento,\n",
    "                \"url_image\": url_image\n",
    "            })\n",
    "        \n",
    "    return datos_productos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eee6954-d58b-4d2c-95b7-bac9b40583b6",
   "metadata": {},
   "source": [
    "### FUNCI√ìN PRINCIPAL DE ORQUESTACI√ìN Y PAGINACI√ìN DIN√ÅMICA\n",
    "Esta funci√≥n act√∫a como el controlador principal del scraper, coordinando la descarga de m√∫ltiples p√°ginas de resultados, la extracci√≥n de datos de cada p√°gina y la navegaci√≥n din√°mica entre p√°ginas mediante enlaces \"Siguiente\". Adem√°s, incorpora validaciones y mensajes de control para detectar errores cr√≠ticos durante el proceso de scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b82f6-a581-4db0-8d8a-4b8cfc40db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. ORQUESTACI√ìN Y PAGINACI√ìN DIN√ÅMICA ---\n",
    "def ejecutar_scraper_amazon_ofertas(url_inicial, num_paginas):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso completo de scraping de ofertas desde Amazon,\n",
    "    gestionando la paginaci√≥n din√°mica y centralizando la l√≥gica de\n",
    "    descarga, extracci√≥n y acumulaci√≥n de los datos recolectados.\n",
    "\n",
    "    Par√°metros:\n",
    "    url_inicial (str): URL inicial de b√∫squeda en Amazon.\n",
    "    num_paginas (int): N√∫mero m√°ximo de p√°ginas a recorrer.\n",
    "\n",
    "    Retorna:\n",
    "    list: Lista consolidada de diccionarios con los datos de todas\n",
    "    las ofertas v√°lidas encontradas durante la ejecuci√≥n.\n",
    "    \"\"\"\n",
    "    total_datos = []\n",
    "    url_actual = url_inicial\n",
    "    \n",
    "    # Bucle principal de paginaci√≥n\n",
    "    for pagina_actual in range(1, num_paginas + 1):\n",
    "        if url_actual is None:\n",
    "            print(\"‚ö†Ô∏è No se encontr√≥ el enlace a la p√°gina siguiente. Finalizando.\")\n",
    "            break\n",
    "            \n",
    "        # Mensaje informativo de progreso\n",
    "        print(f\"\\nüì¢ Procesando p√°gina {pagina_actual}/{num_paginas}. URL actual: {url_actual}\")\n",
    "        \n",
    "        # Obtenci√≥n y parseo del contenido HTML\n",
    "        soup = obtener_contenido_pagina(url_actual)\n",
    "        \n",
    "        if soup is None:\n",
    "            print(\"üõë Error al obtener la p√°gina. Deteniendo el scraper.\")\n",
    "            break\n",
    "            \n",
    "        # Extracci√≥n de datos de productos en oferta\n",
    "        nuevos_datos = extraer_datos_amazon_ofertas(soup) \n",
    "        \n",
    "        # Validaci√≥n cr√≠tica en la primera p√°gina\n",
    "        if not nuevos_datos and pagina_actual == 1:\n",
    "            print(\"‚ö†Ô∏è ¬°FALLO CR√çTICO! No se encontraron productos con oferta en la p√°gina 1.\")\n",
    "            break\n",
    "        \n",
    "        # Acumulaci√≥n de los datos extra√≠dos\n",
    "        total_datos.extend(nuevos_datos)\n",
    "        \n",
    "        # B√∫squeda del enlace a la siguiente p√°gina\n",
    "        enlace_siguiente = soup.find('a', class_='s-pagination-next')\n",
    "\n",
    "        if enlace_siguiente:\n",
    "            url_actual = \"https://www.amazon.com\" + enlace_siguiente.get('href')\n",
    "        else:\n",
    "            url_actual = None \n",
    "\n",
    "        # Delay para respetar los servidores y evitar bloqueos\n",
    "        time.sleep(3) \n",
    "\n",
    "    return total_datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6436ec7-8d89-495c-84e8-359c1e2338c1",
   "metadata": {},
   "source": [
    "### EJECUCI√ìN FINAL DEL SCRAPER Y PRESENTACI√ìN DE RESULTADOS\n",
    "Esta secci√≥n ejecuta el scraper completo cuando el archivo se ejecuta como programa principal. Adem√°s, se encarga de:\n",
    "- Mostrar mensajes de estado del proceso\n",
    "- Imprimir una muestra de los datos obtenidos\n",
    "- Convertir los resultados a un DataFrame\n",
    "- Guardar la informaci√≥n final en un archivo CSV para an√°lisis posterior o trabajo colaborativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc0eda-c5cd-4a0e-a445-bd202e03ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. EJECUCI√ìN E IMPRESI√ìN COMO LISTA DE DICCIONARIOS ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Ejecutar el proceso de extracci√≥n\n",
    "    resultados_finales_diccionario = ejecutar_scraper_amazon_ofertas(URL_INICIAL, NUMERO_DE_PAGINAS)\n",
    "    \n",
    "    # ... (c√≥digo de impresi√≥n y muestra) ...\n",
    "    \n",
    "    # Verificar si se obtuvieron resultados v√°lidos\n",
    "    if resultados_finales_diccionario:\n",
    "        print(f\"\\n‚úÖ Extracci√≥n de Amazon completada. Total de productos filtrados: {len(resultados_finales_diccionario)}.\")\n",
    "        \n",
    "        # Imprimir la salida final en formato de lista de diccionarios\n",
    "        print(\"\\n--- SALIDA FINAL: LISTA DE DICCIONARIOS (Muestra de Ofertas Reales) ---\")\n",
    "        \n",
    "        if len(resultados_finales_diccionario) > 0:\n",
    "            # Imprimir solo una muestra para evitar saturar la consola\n",
    "            print(json.dumps(resultados_finales_diccionario[:5], indent=4))\n",
    "            print(f\"\\n... Se omiten {len(resultados_finales_diccionario) - 5} productos m√°s para la vista previa. Total: {len(resultados_finales_diccionario)}.\")\n",
    "        \n",
    "        # Conversi√≥n de los resultados a DataFrame para an√°lisis\n",
    "        df = pd.DataFrame(resultados_finales_diccionario)\n",
    "        \n",
    "        # Definici√≥n del nombre del archivo CSV de salida\n",
    "        NOMBRE_ARCHIVO_CSV = 'amazon_ofertas_filtradas.csv'\n",
    "        \n",
    "        # Guardar los datos en formato CSV\n",
    "        df.to_csv(NOMBRE_ARCHIVO_CSV, index=False, encoding='utf-8')\n",
    "        print(f\"\\nüíæ Datos guardados en CSV: {NOMBRE_ARCHIVO_CSV}\")\n",
    "        \n",
    "    else:\n",
    "        # Mensaje informativo cuando no se obtienen resultados\n",
    "        print(\"\\n‚ö†Ô∏è La extracci√≥n de Amazon no produjo resultados con los filtros aplicados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c5191-79cf-4e6e-ba39-ef24b0541019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
